{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9824db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'etl_dwh_loader' from 'c:\\\\Users\\\\samaz\\\\Desktop\\\\TFF\\\\MarketReviews_VS_TRUTH\\\\etl_dwh_loader.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyodbc\n",
    "import etl_dwh_loader as etl_loader  \n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Si vous modifiez etl_dwh_loader.py, ré-exécutez cette cellule\n",
    "# pour forcer le rechargement du module\n",
    "importlib.reload(etl_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6064f015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staging -> DWH) started\n",
      "   -> (E) Extraction des données de Staging_Product_Cleansed (Status='pending')...\n",
      "   -> 0 produits trouvés à migrer.\n",
      "\n",
      "--- ✅ Fin : Rien à charger dans le DWH. ---\n",
      "\n",
      "Script terminé.\n"
     ]
    }
   ],
   "source": [
    "print(\"Staging -> DWH) started\")\n",
    "\n",
    "conn_staging_read = None\n",
    "cursor_staging_read = None\n",
    "tasks = []\n",
    "\n",
    "try:\n",
    "    # 1. Connexion initiale au Staging (juste pour lire la liste des tâches)\n",
    "    conn_staging_read = etl_loader.get_staging_connection()\n",
    "    cursor_staging_read = conn_staging_read.cursor()\n",
    "    \n",
    "    # 2. Extraire les tâches (E)\n",
    "    tasks = etl_loader.get_pending_cleansed_data(cursor_staging_read)\n",
    "    \n",
    "    cursor_staging_read.close()\n",
    "    conn_staging_read.close()\n",
    "    \n",
    "    if not tasks:\n",
    "        print(\"\\n--- ✅ Fin : Rien à charger dans le DWH. ---\")\n",
    "    else:\n",
    "        print(f\"\\n--- {len(tasks)} produits à charger trouvés. Lancement de la boucle... ---\")\n",
    "\n",
    "    # 3. Boucle de Transformation (T) et Chargement (L)\n",
    "    # On traite chaque produit comme sa propre transaction\n",
    "    for task in tasks:\n",
    "        cleansed_id = task['CleansedProductID']\n",
    "        sku = task['SKU']\n",
    "        \n",
    "        # On a besoin de deux connexions par boucle pour gérer les transactions séparément\n",
    "        conn_dwh = None\n",
    "        cursor_dwh = None\n",
    "        conn_staging_write = None\n",
    "        cursor_staging_write = None\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n--- Traitement du SKU {sku} (CleansedID: {cleansed_id}) ---\")\n",
    "            \n",
    "            # --- Transaction DWH ---\n",
    "            conn_dwh = etl_loader.get_dwh_connection()\n",
    "            cursor_dwh = conn_dwh.cursor()\n",
    "            \n",
    "            # 3a. Gérer la Dimension (SCD Type 1)\n",
    "            product_key = etl_loader.load_dim_product_scd1(cursor_dwh, task)\n",
    "            \n",
    "            # 3b. Gérer les Faits\n",
    "            date_key, time_key = etl_loader.get_date_time_keys(task['ScrapeTimestamp'])\n",
    "            etl_loader.load_fact_snapshot(cursor_dwh, task, product_key, date_key, time_key)\n",
    "            \n",
    "            conn_dwh.commit() # Si tout s'est bien passé dans le DWH, on valide\n",
    "            print(f\"   -> ✅ SKU {sku} chargé dans le DWH.\")\n",
    "\n",
    "            # --- Transaction Staging ---\n",
    "            # Si le DWH est OK, on met à jour le statut du Staging\n",
    "            conn_staging_write = etl_loader.get_staging_connection()\n",
    "            cursor_staging_write = conn_staging_write.cursor()\n",
    "            \n",
    "            etl_loader.update_staging_status(cursor_staging_write, cleansed_id, 'processed')\n",
    "            conn_staging_write.commit() # Valide le statut 'processed'\n",
    "\n",
    "        except Exception as e:\n",
    "            # Gérer une erreur pour ce produit spécifique\n",
    "            print(f\"   -> ❌ ERREUR sur SKU {sku}: {e}.\")\n",
    "            if conn_dwh: conn_dwh.rollback()     # Annuler les changements dans le DWH\n",
    "            if conn_staging_write: conn_staging_write.rollback() # Annuler\n",
    "            \n",
    "            # Marquer comme 'failed' (dans une transaction séparée)\n",
    "            conn_fail = etl_loader.get_staging_connection()\n",
    "            cursor_fail = conn_fail.cursor()\n",
    "            etl_loader.update_staging_status(cursor_fail, cleansed_id, 'failed')\n",
    "            conn_fail.commit()\n",
    "            cursor_fail.close()\n",
    "            conn_fail.close()\n",
    "\n",
    "        finally:\n",
    "            if cursor_staging_write: cursor_staging_write.close()\n",
    "            if conn_staging_write: conn_staging_write.close()\n",
    "            if cursor_dwh: cursor_dwh.close()\n",
    "            if conn_dwh: conn_dwh.close()\n",
    "\n",
    "except Exception as e_main:\n",
    "    print(f\"\\n--- ❌ ERREUR MAJEURE du Pipeline : {e_main} ---\")\n",
    "finally:\n",
    "    # Au cas où la connexion de lecture initiale planterait\n",
    "    print(\"\\nScript terminé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af1eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32a574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
